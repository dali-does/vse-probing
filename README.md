# Probing Multimodal Embeddings for Linguistic Properties: the Visual-Semantic Case

This repository contains material and code related to the COLING 2020-paper `Probing Multimodal Embeddings for Linguistic Properties: the Visual Semantic Case`.

## Contact

All contact regarding this repository or the related paper is pointed to `dali@cs.umu.se`.


# Instructions - under construction

The code runs on Pytorch and is partially based on [VSE++](https://github.com/fartashf/vsepp/).

## External dependencies

* VSE++
* VSE-C
* HAL
* MS-COCO

# Training and evaluation
Extracting embeddings, training and evaluating the probe are all done by `evaluate.py`. An example call is
```
python3 evaluate.py --annotation_path annotations/ --data_path vsepp/data/ --model_path vsepp/runs/coco_vse++/model_best.pth.tar --model vsepp --task objcat --probe linear --resultfile results/vse_linear_objcat
```
## Directory structure

1. The symbolic link `data` is used by the VSE-models to load the vocabulary.
2. The `vsepp` data directory contains the alternative caption files generated by `transform.py`. These generated files needs to be either linked or copied to the HAL and VSE_C data directories.

```
.
├── LICENSE
├── README.md
├── embeddings.py
├── evaluate.py
├── log_collector.py
├── probing_model.py
├── probing_tasks.py
├── progress.py
├── requirements.txt
├── transform.py
├── annotations
│   ├── alt_captions_train2014.json
│   ├── alt_captions_val2014.json
│   ├── alts_train2014.json
│   ├── captions_train2014.json
│   ├── captions_val2014.json
│   ├── instances_train2014.json
│   ├── instances_val2014.json
│   └── train_ids_categories.npy
├── data -> vsepp/data/
├── results/
├── hal/
├── VSE_C/
└── vsepp
    ├── LICENSE
    ├── README.md
    ├── data
    │   ├── coco_precomp
    │   │   ├── dev.txt
    │   │   ├── dev_alts.txt
    │   │   ├── dev_caps.txt
    │   │   ├── dev_ims.npy
    │   │   ├── dev_tags.txt
    │   │   ├── test.txt
    │   │   ├── test_alts.txt
    │   │   ├── test_caps.txt
    │   │   ├── test_ims.npy
    │   │   ├── test_tags.txt
    │   │   ├── train.txt
    │   │   ├── train_alts.txt
    │   │   ├── train_caps.txt
    │   │   ├── train_ims.npy
    │   │   └── train_tags.txt
    │   └── vocab
    │       └── coco_precomp_vocab.pkl
    ├── model.py
    ├── runs
    │   └── coco_vse++
    │       └── model_best.pth.tar
    └── vocab.py
10 directories, 27 files
```


## Detailed usage
```
usage: evaluate.py  [-h] [--vsemodel {vsepp,vsec,hal}] [--unimodel {bert,gpt2}]
                    [--annotation_path ANNOTATION_PATH] [--data_path DATA_PATH]
                    [--split {train2014,val2014}]
                    [--vse_model_path VSE_MODEL_PATH]
                    [--result_file RESULT_FILE]
                    [--task {objcat,numobj,semcong}] [--seed SEED]
                    [--probe {mlp,linear}]

optional arguments:
  -h, --help            show this help message and exit
  --vsemodel {vsepp,vsec,hal}
                        The visual-semantic embedding model to probe.
  --unimodel {bert,gpt2}
                        The unimodal embedding model to probe.
  --annotation_path ANNOTATION_PATH
                        Path to MSCOCO annotations.
  --data_path DATA_PATH
                        Path to the raw MSCOCO data.
  --split {train2014,val2014}
                        Which MSCOCO datasplit to use.
  --vse_model_path VSE_MODEL_PATH
                        Path to pretrained visual-semantic embedding model.
  --result_file RESULT_FILE
                        File to store probing results.
  --task {objcat,numobj,semcong}
                        The probing task to execute.
  --seed SEED           The seed used for the Numpy RNG.
  --probe {mlp,linear}  Which probing model to use.
```

